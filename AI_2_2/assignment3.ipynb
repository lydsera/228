{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498d70fa",
   "metadata": {},
   "source": [
    "# 线性回归（二）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226badd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2906a",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">1) 使用pandas库的read_csv()函数(可以参考[pandas的官方文档](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html))将训练数据集'train.csv'和测试数据集'test.csv'载入到Dataframe对象中。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f59640f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#读取数据集\n",
    "train_frame = pd.read_csv('train.csv')\n",
    "test_frame = pd.read_csv('test.csv')\n",
    "\n",
    "#转化成numpy矩阵\n",
    "train = np.array(train_frame)\n",
    "test = np.array(test_frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bccb84d",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">2) 假设模型为一元线性回归模型$\\hat{y}=wx+b$, 损失函数为$l(w,b)=\\frac{1}{2}\\sum_{i=1}^m(\\hat{y}^{(i)}-y^{(i)})^2$, 其中$\\hat{y}^{(i)}$表示第$i$个样本的预测值，$y^{(i)}$表示第$i$个样本的实际标签值, $m$为训练集中样本的个数。求出使得损失函数最小化的参数$w$和$b$。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011976d",
   "metadata": {},
   "source": [
    "方法① \n",
    "\n",
    "将$l(w,b)$分别对$w$和$b$求导，得到\n",
    "$$\n",
    "\\frac{\\partial l(w,b)}{\\partial w}=w\\sum_{i=1}^m x_i^2 -\\sum_{i=1}^m (y_i-b)x_i,\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial l(w,b)}{\\partial b}=mb -\\sum_{i=1}^m (y_i-wx_i),\n",
    "$$\n",
    "令上述两式为零即可得到$w$和$b$的解析解：\n",
    "$$\n",
    "w=\\frac{\\sum_{i=1}^m y_i (x_i-\\bar{x})}{\\sum_{i=1}^m x_i^2-\\frac{1}{m}(\\sum_{i=1}^m x_i)^2},\n",
    "$$\n",
    "$$\n",
    "b=\\frac{1}{m}\\sum_{i=1}^m(y_i-wx_i),\n",
    "$$\n",
    "其中$\\bar{x}=\\frac{1}{m}\\sum_{i=1}^m x_i$为$x$的均值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04bc78",
   "metadata": {},
   "source": [
    "方法② 梯度下降法。手动实现梯度下降法(不使用机器学习框架，如PyTorch、TensorFlow等)来进行模型的训练。算法步骤如下：(a)初始化模型参数$w$和$b$的值；(b)在负梯度的方向上更新参数(批量梯度下降($\\left|B\\right|=m$)、小批量随机梯度下降或者随机梯度下降($\\left|B\\right|=1$)均可)，并不断迭代这一步骤，更新公式(以小批量随机梯度下降为例)可以写成：$$w\\gets w-\\frac{\\eta}{\\left|B\\right|}\\sum_{i\\in{B}}x^{(i)}(wx^{(i)}+b-y^{(i)})$$, 和$$b\\gets b-\\frac{\\eta}{\\left|B\\right|}\\sum_{i\\in{B}}(wx^{(i)}+b-y^{(i)})$$， 其中$\\eta$表示学习率,$B$表示每次迭代中随机抽样的小批量，$\\left|B\\right|$则表示$B$中的样本数量。(c) 终止条件为迭代次数达到某一上限或者参数更新的幅度小于某个阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad649ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "w=0\n",
    "b=0\n",
    "ita=0.01\n",
    "x=train[:,0]\n",
    "y=train[:,1]\n",
    "B=len(x)\n",
    "for i in range(1000):\n",
    "    w_tem=w-ita/B*np.sum(x*(w*x+b-y))\n",
    "    b_tem=b-ita/B*np.sum(w*x+b-y)\n",
    "    w=w_tem\n",
    "    b=b_tem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f2a87",
   "metadata": {},
   "source": [
    "方法③ \n",
    "\n",
    "用矩阵表示，假设数据集有$m$个样本，特征有$n$维$。X=\\left[ \\begin{matrix} x_{11} & x_{12} & \\cdots & x_{1n} & 1 \\\\\n",
    "                         x_{21} & x_{22} & \\cdots & x_{2n} & 1 \\\\\n",
    "                         \\vdots & \\vdots &      & \\vdots & \\vdots \\\\\n",
    "                         x_{m1} & x_{m2} & \\cdots & x_{mn} & 1 \\end{matrix} \\right]$,\n",
    "        实际标签$Y=\\left[ \\begin{matrix} y_{1} \\\\\n",
    "                         y_{2} \\\\\n",
    "                         \\vdots \\\\\n",
    "                         y_{m}\\end{matrix} \\right]$,\n",
    "        参数$B=\\left[ \\begin{matrix} w_{1} \\\\\n",
    "                         w_{2} \\\\\n",
    "                         \\vdots \\\\\n",
    "                         w_{n} \\\\\n",
    "                         b\\end{matrix} \\right]$，则解析解为$B^*=(X^T X)^{-1}X^T Y$。推导过程可参考[这篇文章](https://zhuanlan.zhihu.com/p/74157986)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16c177",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">3) 使用求解出来的线性回归模型对测试数据集'test.csv'进行预测，输出可视化结果（比如用seaborn或者matplotlib等可视化库来画出测试数据的散点图以及训练好的模型函数图像）。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19a5413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "#A=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "#x = A[0, :] #从一个矩阵中提取出一行作为一个向量\n",
    "#y1 = np.array([2, 3, 5])\n",
    "#plt.plot(x, y1) #画出折线图\n",
    "#y2 = np.array([2.5, 2.8, 5.3])\n",
    "#plt.plot(x, y2, '.') #画出散点图\n",
    "#plt.show()\n",
    "x_test=test[:,0]\n",
    "y_test=test[:,1]\n",
    "y_fore=w*x_test+b\n",
    "plt.plot(x_test,y_fore)\n",
    "plt.plot(x_test,y_test,'.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227579c",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">4) 在训练数据集'train2.csv'上求一个三元线性回归模型$\\hat{y}=w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3$的使得损失函数$l(w_0,w_1,w_2,w_3)=\\frac{1}{2}\\sum_{i=1}^m(\\hat{y}^{(i)}-y^{(i)})^2$最小的参数$w_0,w_1,w_2$以及$w_3$。并在测试数据集'test2.csv'上进行预测，输出预测结果的均方误差$MSE(\\hat{y},y)=\\frac{1}{n}\\sum^n_{i=1}(y^{(i)}-\\hat{y}^{(i)})^2$, $n$为测试集中样本个数。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f6178",
   "metadata": {},
   "source": [
    "方法① 同2)中的方法③。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea8a93",
   "metadata": {},
   "source": [
    "方法② 类似2)中的方法②。算法步骤如下：(a)初始化模型参数$w_0,w_1,w_2,w_3$的值；(b)在负梯度的方向上更新参数(批量梯度下降($\\left|B\\right|=m$)、小批量随机梯度下降或者随机梯度下降($\\left|B\\right|=1$)均可)，并不断迭代这一步骤，更新公式(以小批量随机梯度下降为例)可以写成：$$w_j\\gets w_j-\\frac{\\eta}{\\left|B\\right|}\\sum_{i\\in{B}}x_j^{(i)}(w_0 + w_1 x_1^{(i)}+w_2 x_2^{(i)}+w_3 x_3^{(i)}-y^{(i)}), j=0,1,2,3$$, 其中$x_0^{(i)}=1$，$\\eta$表示学习率,$B$表示每次迭代中随机抽样的小批量，$\\left|B\\right|$则表示$B$中的样本数量。(c) 终止条件为迭代次数达到某一上限或者参数更新的幅度小于某个阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2d062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame2 = pd.read_csv('train2.csv')\n",
    "test_frame2 = pd.read_csv('test2.csv')\n",
    "\n",
    "\n",
    "train2 = np.array(train_frame2)\n",
    "test2 = np.array(test_frame2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "115f58e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1653734559851884\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "X=train2[:,0:3]\n",
    "Y=train2[:,3:4]\n",
    "tem=np.ones(len(X))\n",
    "X=np.c_[X,tem]\n",
    "ita=0.01\n",
    "W=np.array([0,0,0,0]).reshape(4,1)\n",
    "B=len(X)\n",
    "for i in range(10000):\n",
    "    W=W-(ita/B*np.sum(X*(X.dot(W)-Y),axis=0)).reshape(4,1)\n",
    "X_test=test2[:,0:3]\n",
    "tem1=np.ones(len(X_test))\n",
    "X_test=np.c_[X_test,tem1]\n",
    "Y_test=test2[:,3:4]\n",
    "Y_fore=X_test.dot(W)\n",
    "Y_tem=Y_test-Y_fore\n",
    "MS=np.sum(Y_tem**2)/len(Y_test)\n",
    "print(MS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
